{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeea1e8a-ab0a-445d-8843-72bbf1dce41a",
   "metadata": {},
   "source": [
    "## The Hetionet data contains the source node and a target node with the kind of relation that exist between them. let's start by extracting the nodes IDs for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57e12e27-086b-497f-9c13-84980688d7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        source metaedge                          target\n",
      "0   Gene::9021     GpBP  Biological Process::GO:0071357\n",
      "1  Gene::51676     GpBP  Biological Process::GO:0098780\n",
      "2     Gene::19     GpBP  Biological Process::GO:0055088\n",
      "3   Gene::3176     GpBP  Biological Process::GO:0010243\n",
      "4   Gene::3039     GpBP  Biological Process::GO:0006898\n",
      "                        id                       name     kind\n",
      "0  Anatomy::UBERON:0000002             uterine cervix  Anatomy\n",
      "1  Anatomy::UBERON:0000004                       nose  Anatomy\n",
      "2  Anatomy::UBERON:0000006        islet of Langerhans  Anatomy\n",
      "3  Anatomy::UBERON:0000007            pituitary gland  Anatomy\n",
      "4  Anatomy::UBERON:0000010  peripheral nervous system  Anatomy\n",
      "                          metaedge abbreviation   edges  source_nodes  \\\n",
      "0   Anatomy - downregulates - Gene          AdG  102240            36   \n",
      "1       Anatomy - expresses - Gene          AeG  526407           241   \n",
      "2     Anatomy - upregulates - Gene          AuG   97848            36   \n",
      "3          Compound - binds - Gene          CbG   11571          1389   \n",
      "4  Compound - causes - Side Effect         CcSE  138944          1071   \n",
      "\n",
      "   target_nodes  unbiased  \n",
      "0         15097    102240  \n",
      "1         18094    453477  \n",
      "2         15929     97848  \n",
      "3          1689         0  \n",
      "4          5701         0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "edge_data = pd.read_csv(\"data/hetionet-v1.0-edges.sif\", sep = \"\\t\")\n",
    "node_data = pd.read_csv(\"data/hetionet-v1.0-nodes.tsv\", sep = \"\\t\")\n",
    "meta_edge_data = pd.read_csv(\"data/metaedges.tsv\", sep = \"\\t\")\n",
    "print(edge_data.head(5))\n",
    "print(node_data.head(5))\n",
    "print(meta_edge_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53064850-398c-47f9-88b2-7b98b2142a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>metaedge</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gene::9021</td>\n",
       "      <td>GpBP</td>\n",
       "      <td>Biological Process::GO:0071357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gene::51676</td>\n",
       "      <td>GpBP</td>\n",
       "      <td>Biological Process::GO:0098780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gene::19</td>\n",
       "      <td>GpBP</td>\n",
       "      <td>Biological Process::GO:0055088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gene::3176</td>\n",
       "      <td>GpBP</td>\n",
       "      <td>Biological Process::GO:0010243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gene::3039</td>\n",
       "      <td>GpBP</td>\n",
       "      <td>Biological Process::GO:0006898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source metaedge                          target\n",
       "0   Gene::9021     GpBP  Biological Process::GO:0071357\n",
       "1  Gene::51676     GpBP  Biological Process::GO:0098780\n",
       "2     Gene::19     GpBP  Biological Process::GO:0055088\n",
       "3   Gene::3176     GpBP  Biological Process::GO:0010243\n",
       "4   Gene::3039     GpBP  Biological Process::GO:0006898"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77819b9e-7061-4621-a3e9-f47fc8e23411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def create_entity_relation_dicts_from_df(data_path, triples_df):\n",
    "    entity_set = set(triples_df['source']).union(set(triples_df['target']))\n",
    "    relation_set = set(triples_df['metaedge'])\n",
    "\n",
    "    entity2id = {entity: idx for idx, entity in enumerate(entity_set)}\n",
    "    relation2id = {relation: idx for idx, relation in enumerate(relation_set)}\n",
    "\n",
    "    with open(os.path.join(data_path, 'entities.dict'), 'w') as fout:\n",
    "        for entity, idx in entity2id.items():\n",
    "            fout.write(f\"{idx}\\t{entity}\\n\")\n",
    "\n",
    "    with open(os.path.join(data_path, 'relations.dict'), 'w') as fout:\n",
    "        for relation, idx in relation2id.items():\n",
    "            fout.write(f\"{idx}\\t{relation}\\n\")\n",
    "\n",
    "    return entity2id, relation2id\n",
    "\n",
    "# Example usage:\n",
    "data_path = 'data/FB15k'\n",
    "triples_df = pd.read_csv(\"data/FB15k/hetionet-v1.0-edges.sif\", delimiter=\"\\t\")\n",
    "entity2id, relation2id = create_entity_relation_dicts_from_df(data_path, triples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e731146-72b7-4509-9b5b-3bcf7c5b6fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_data, test_data = train_test_split(edge_data, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to save DataFrame to a text file\n",
    "def save_to_text_file(df, file_name):\n",
    "    df.to_csv(file_name, sep='\\t', index=False, header = False)\n",
    "\n",
    "# Save train, validation, and test datasets to text files\n",
    "path = \"data/FB15k/\"\n",
    "save_to_text_file(train_data, path + 'train.txt')\n",
    "save_to_text_file(val_data, path + 'valid.txt')\n",
    "save_to_text_file(test_data, path + 'test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f1968e-f160-4df8-95b7-d938376d14b2",
   "metadata": {},
   "source": [
    "# Descriptive Statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b25ef74-346f-47ac-9c93-628247f31015",
   "metadata": {},
   "source": [
    "# Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5783e09f-fbd0-49fc-a3e8-47b19a19f342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes: 47031\n",
      "Number of unique node types: 11\n",
      "----------------------------------------\n",
      "Anatomy\n",
      "Biological Process\n",
      "Cellular Component\n",
      "Compound\n",
      "Disease\n",
      "Gene\n",
      "Molecular Function\n",
      "Pathway\n",
      "Pharmacologic Class\n",
      "Side Effect\n",
      "Symptom\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of nodes: {len(node_data)}\")\n",
    "print(\"Number of unique node types: {}\".format(len(node_data[\"kind\"].unique())))\n",
    "print(\"-\"*40)\n",
    "for node_type in node_data[\"kind\"].unique():\n",
    "    print(node_type)\n",
    "print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b20ac2e-14d3-4b87-a923-be668a7258a5",
   "metadata": {},
   "source": [
    "# Convert dataset to tripples format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f2623c8-646a-40ea-950b-7fd27fc1dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import KGEModel\n",
    "from dataloader import TrainDataset, BidirectionalOneShotIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fefb32-dc45-4b7f-801a-69bcfdf4b2be",
   "metadata": {},
   "source": [
    "# Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3be68b3a-868d-414f-afba-147314ee6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.cuda = False\n",
    "        self.do_train = True\n",
    "        self.do_valid = True\n",
    "        self.do_test = True\n",
    "        self.evaluate_train = False\n",
    "        self.countries = False\n",
    "        self.regions = None\n",
    "        self.data_path = 'data/FB15k'\n",
    "        self.model = 'TransE'\n",
    "        self.double_entity_embedding = False\n",
    "        self.double_relation_embedding = False\n",
    "        self.negative_sample_size = 128\n",
    "        self.hidden_dim = 200\n",
    "        self.gamma = 12.0\n",
    "        self.negative_adversarial_sampling = False\n",
    "        self.adversarial_temperature = 1.0\n",
    "        self.batch_size = 1024\n",
    "        self.regularization = 0.0\n",
    "        self.test_batch_size = 4\n",
    "        self.uni_weight = False\n",
    "        self.learning_rate = 0.0001\n",
    "        self.cpu_num = 10\n",
    "        self.init_checkpoint = None\n",
    "        self.save_path = 'models/TransE_FB15k'\n",
    "        self.max_steps = 100\n",
    "        self.warm_up_steps = None\n",
    "        self.save_checkpoint_steps = 1000\n",
    "        self.valid_steps = 1000\n",
    "        self.log_steps = 100\n",
    "        self.test_log_steps = 1000\n",
    "        self.nentity = 0\n",
    "        self.nrelation = 0\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab04fe9-2398-4f47-9585-259302d8db01",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ad33eab-2bbb-41c2-92ba-c0db1c400347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_triple(file_path, entity2id, relation2id):\n",
    "    triples = []\n",
    "    with open(file_path) as fin:\n",
    "        for line in fin:\n",
    "            head, relation, tail = line.strip().split('\\t')\n",
    "            triples.append((entity2id[head], relation2id[relation], entity2id[tail]))\n",
    "    return triples\n",
    "\n",
    "def set_logger(args):\n",
    "    log_file = os.path.join(args.save_path or args.init_checkpoint, 'train.log')\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s %(message)s',\n",
    "        level=logging.INFO,\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def log_metrics(mode, step, metrics):\n",
    "    for metric in metrics:\n",
    "        logging.info('%s %s at step %d: %f' % (mode, metric, step, metrics[metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364981f0-4dfd-4604-9f87-1dd415f5d6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 02:37:11,715 Model: TransE\n",
      "2024-06-02 02:37:11,762 Data Path: data/FB15k\n",
      "2024-06-02 02:37:11,776 #entity: 45158\n",
      "2024-06-02 02:37:11,780 #relation: 24\n",
      "2024-06-02 02:37:21,758 #train: 1440125\n",
      "2024-06-02 02:37:24,662 #valid: 360032\n",
      "2024-06-02 02:37:25,796 #test: 450040\n",
      "2024-06-02 02:37:26,613 Model Parameter Configuration:\n",
      "2024-06-02 02:37:26,678 Parameter gamma: torch.Size([1]), require_grad = False\n",
      "2024-06-02 02:37:26,680 Parameter embedding_range: torch.Size([1]), require_grad = False\n",
      "2024-06-02 02:37:26,694 Parameter entity_embedding: torch.Size([45158, 200]), require_grad = True\n",
      "2024-06-02 02:37:26,695 Parameter relation_embedding: torch.Size([24, 200]), require_grad = True\n",
      "2024-06-02 02:38:32,592 Randomly Initializing TransE Model...\n",
      "2024-06-02 02:38:32,592 Start Training...\n",
      "2024-06-02 02:38:32,614 init_step = 0\n",
      "2024-06-02 02:38:32,621 batch_size = 1024\n",
      "2024-06-02 02:38:32,623 negative_adversarial_sampling = 0\n",
      "2024-06-02 02:38:32,626 hidden_dim = 200\n",
      "2024-06-02 02:38:32,629 gamma = 12.000000\n",
      "2024-06-02 02:38:32,633 negative_adversarial_sampling = False\n",
      "2024-06-02 02:38:32,633 learning_rate = 0\n",
      "2024-06-02 02:43:26,799 Training average positive_sample_loss at step 0: 0.446781\n",
      "2024-06-02 02:43:27,168 Training average negative_sample_loss at step 0: 1.130610\n",
      "2024-06-02 02:43:27,211 Training average loss at step 0: 0.788695\n",
      "2024-06-02 02:43:27,226 Evaluating on Valid Dataset...\n",
      "2024-06-02 02:46:18,565 Evaluating the model... (0/180016)\n",
      "2024-06-02 02:54:53,039 Evaluating the model... (1000/180016)\n",
      "2024-06-02 03:03:09,715 Evaluating the model... (2000/180016)\n",
      "2024-06-02 03:11:19,205 Evaluating the model... (3000/180016)\n",
      "2024-06-02 06:19:33,277 Evaluating the model... (4000/180016)\n"
     ]
    }
   ],
   "source": [
    "def override_config(args):\n",
    "    # Implement the logic to override config based on checkpoint\n",
    "    pass\n",
    "\n",
    "def save_model(model, optimizer, save_variable_list, args):\n",
    "    # Implement the logic to save the model\n",
    "    pass\n",
    "def main(args):\n",
    "    if (not args.do_train) and (not args.do_valid) and (not args.do_test):\n",
    "        raise ValueError('One of train/val/test mode must be chosen.')\n",
    "    \n",
    "    if args.init_checkpoint:\n",
    "        override_config(args)\n",
    "    elif args.data_path is None:\n",
    "        raise ValueError('One of init_checkpoint/data_path must be chosen.')\n",
    "\n",
    "    if args.do_train and args.save_path is None:\n",
    "        raise ValueError('Where do you want to save your trained model?')\n",
    "    \n",
    "    if args.save_path and not os.path.exists(args.save_path):\n",
    "        os.makedirs(args.save_path)\n",
    "    \n",
    "    set_logger(args)\n",
    "    \n",
    "    with open(os.path.join(args.data_path, 'entities.dict')) as fin:\n",
    "        entity2id = dict()\n",
    "        for line in fin:\n",
    "            eid, entity = line.strip().split('\\t')\n",
    "            entity2id[entity] = int(eid)\n",
    "\n",
    "    with open(os.path.join(args.data_path, 'relations.dict')) as fin:\n",
    "        relation2id = dict()\n",
    "        for line in fin:\n",
    "            rid, relation = line.strip().split('\\t')\n",
    "            relation2id[relation] = int(rid)\n",
    "    \n",
    "    if args.countries:\n",
    "        regions = list()\n",
    "        with open(os.path.join(args.data_path, 'regions.list')) as fin:\n",
    "            for line in fin:\n",
    "                region = line.strip()\n",
    "                regions.append(entity2id[region])\n",
    "        args.regions = regions\n",
    "\n",
    "    nentity = len(entity2id)\n",
    "    nrelation = len(relation2id)\n",
    "    \n",
    "    args.nentity = nentity\n",
    "    args.nrelation = nrelation\n",
    "    \n",
    "    logging.info('Model: %s' % args.model)\n",
    "    logging.info('Data Path: %s' % args.data_path)\n",
    "    logging.info('#entity: %d' % nentity)\n",
    "    logging.info('#relation: %d' % nrelation)\n",
    "    \n",
    "    train_triples = read_triple(os.path.join(args.data_path, 'train.txt'), entity2id, relation2id)\n",
    "    logging.info('#train: %d' % len(train_triples))\n",
    "    valid_triples = read_triple(os.path.join(args.data_path, 'valid.txt'), entity2id, relation2id)\n",
    "    logging.info('#valid: %d' % len(valid_triples))\n",
    "    test_triples = read_triple(os.path.join(args.data_path, 'test.txt'), entity2id, relation2id)\n",
    "    logging.info('#test: %d' % len(test_triples))\n",
    "    \n",
    "    all_true_triples = train_triples + valid_triples + test_triples\n",
    "    \n",
    "    kge_model = KGEModel(\n",
    "        model_name=args.model,\n",
    "        nentity=nentity,\n",
    "        nrelation=nrelation,\n",
    "        hidden_dim=args.hidden_dim,\n",
    "        gamma=args.gamma,\n",
    "        double_entity_embedding=args.double_entity_embedding,\n",
    "        double_relation_embedding=args.double_relation_embedding\n",
    "    )\n",
    "    \n",
    "    logging.info('Model Parameter Configuration:')\n",
    "    for name, param in kge_model.named_parameters():\n",
    "        logging.info('Parameter %s: %s, require_grad = %s' % (name, str(param.size()), str(param.requires_grad)))\n",
    "\n",
    "    if args.cuda:\n",
    "        kge_model = kge_model.cuda()\n",
    "    \n",
    "    if args.do_train:\n",
    "        train_dataloader_head = DataLoader(\n",
    "            TrainDataset(train_triples, nentity, nrelation, args.negative_sample_size, 'head-batch'), \n",
    "            batch_size=args.batch_size*2,\n",
    "            shuffle=True, \n",
    "            num_workers=max(1, args.cpu_num//2),\n",
    "            collate_fn=TrainDataset.collate_fn\n",
    "        )\n",
    "        \n",
    "        train_dataloader_tail = DataLoader(\n",
    "            TrainDataset(train_triples, nentity, nrelation, args.negative_sample_size, 'tail-batch'), \n",
    "            batch_size=args.batch_size*2,\n",
    "            shuffle=True, \n",
    "            num_workers=max(1, args.cpu_num//2),\n",
    "            collate_fn=TrainDataset.collate_fn\n",
    "        )\n",
    "        \n",
    "        train_iterator = BidirectionalOneShotIterator(train_dataloader_head, train_dataloader_tail)\n",
    "        \n",
    "        current_learning_rate = args.learning_rate\n",
    "        optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, kge_model.parameters()), \n",
    "            lr=current_learning_rate\n",
    "        )\n",
    "        if args.warm_up_steps:\n",
    "            warm_up_steps = args.warm_up_steps\n",
    "        else:\n",
    "            warm_up_steps = args.max_steps // 2\n",
    "\n",
    "    if args.init_checkpoint:\n",
    "        logging.info('Loading checkpoint %s...' % args.init_checkpoint)\n",
    "        checkpoint = torch.load(os.path.join(args.init_checkpoint, 'checkpoint'))\n",
    "        init_step = checkpoint['step']\n",
    "        kge_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        if args.do_train:\n",
    "            current_learning_rate = checkpoint['current_learning_rate']\n",
    "            warm_up_steps = checkpoint['warm_up_steps']\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    else:\n",
    "        logging.info('Randomly Initializing %s Model...' % args.model)\n",
    "        init_step = 0\n",
    "\n",
    "    #Training loop\n",
    "    step = init_step\n",
    "    #logging initial parameters\n",
    "    logging.info('Start Training...')\n",
    "    logging.info('init_step = %d' % init_step)\n",
    "    logging.info('batch_size = %d' % args.batch_size)\n",
    "    logging.info('negative_adversarial_sampling = %d' % args.negative_adversarial_sampling)\n",
    "    logging.info('hidden_dim = %d' % args.hidden_dim)\n",
    "    logging.info('gamma = %f' % args.gamma)\n",
    "    logging.info('negative_adversarial_sampling = %s' % str(args.negative_adversarial_sampling))\n",
    "    if args.negative_adversarial_sampling:\n",
    "        logging.info('adversarial_temperature = %f' % args.adversarial_temperature)\n",
    "\n",
    "    #Checking training condition\n",
    "    if args.do_train:\n",
    "        logging.info('learning_rate = %d' % current_learning_rate)\n",
    "\n",
    "        training_logs = []\n",
    "\n",
    "        #training loop initialise from initial stem to maximum step\n",
    "        for step in range(init_step, args.max_steps):\n",
    "            #Trianing step: calls the training model in a single step and return the log metrics\n",
    "            log = kge_model.train_step(kge_model, optimizer, train_iterator, args)\n",
    "            \n",
    "            training_logs.append(log)\n",
    "            \n",
    "            #learning rate adjustment\n",
    "           # if step >= warm_up_steps: #warm up steps should always be lower\n",
    "           #     current_learning_rate = current_learning_rate / 10\n",
    "           #     logging.info('Change learning_rate to %f at step %d' % (current_learning_rate, step))\n",
    "           #     #creates a new adam optimizer with the new leatning rate\n",
    "           #     optimizer = torch.optim.Adam(\n",
    "           #         filter(lambda p: p.requires_grad, kge_model.parameters()), \n",
    "           #         lr=current_learning_rate\n",
    "           #     )\n",
    "           #     warm_up_steps = warm_up_steps * 3\n",
    "            #save checkpoints\n",
    "            if step % args.save_checkpoint_steps == 0:\n",
    "                save_variable_list = {\n",
    "                    'step': step, \n",
    "                    'current_learning_rate': current_learning_rate,\n",
    "                    'warm_up_steps': warm_up_steps\n",
    "                }\n",
    "                save_model(kge_model, optimizer, save_variable_list, args)\n",
    "                \n",
    "            if step % args.log_steps == 0:\n",
    "                metrics = {}\n",
    "                for metric in training_logs[0].keys():\n",
    "                    metrics[metric] = sum([log[metric] for log in training_logs])/len(training_logs)\n",
    "                log_metrics('Training average', step, metrics)\n",
    "                training_logs = []\n",
    "                \n",
    "            if args.do_valid and step % args.valid_steps == 0:\n",
    "                logging.info('Evaluating on Valid Dataset...')\n",
    "                metrics = kge_model.test_step(kge_model, valid_triples, all_true_triples, args)\n",
    "                log_metrics('Valid', step, metrics)\n",
    "        \n",
    "        save_variable_list = {\n",
    "            'step': step, \n",
    "            'current_learning_rate': current_learning_rate,\n",
    "            'warm_up_steps': warm_up_steps\n",
    "        }\n",
    "        save_model(kge_model, optimizer, save_variable_list, args)\n",
    "        \n",
    "    if args.do_valid:\n",
    "        logging.info('Evaluating on Valid Dataset...')\n",
    "        metrics = kge_model.test_step(kge_model, valid_triples, all_true_triples, args)\n",
    "        log_metrics('Valid', step, metrics)\n",
    "    \n",
    "    if args.do_test:\n",
    "        logging.info('Evaluating on Test Dataset...')\n",
    "        metrics = kge_model.test_step(kge_model, test_triples, all_true_triples, args)\n",
    "        log_metrics('Test', step, metrics)\n",
    "    \n",
    "    if args.evaluate_train:\n",
    "        logging.info('Evaluating on Training Dataset...')\n",
    "        metrics = kge_model.test_step(kge_model, train_triples, all_true_triples, args)\n",
    "        log_metrics('Test', step, metrics)\n",
    "\n",
    "# Initialize the argument class\n",
    "args = Args()\n",
    "\n",
    "# Run the main function with these arguments\n",
    "main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
